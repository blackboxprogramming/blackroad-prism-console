version: "3.9"

services:
  api:
    build: ./server
    container_name: lucidia_api
    ports: ["7000:7000"]
    environment:
      OLLAMA_HOST: "http://ollama:11434"
      IMAGE_BACKEND: "diffusers"   # or "comfyui"
      DIFFUSERS_MODEL: "stabilityai/stable-diffusion-xl-base-1.0"
      DIFFUSERS_REFINER: "stabilityai/stable-diffusion-xl-refiner-1.0"
      HF_HOME: "/models/hf"
      TORCH_DEVICE: "cuda"         # or "cpu"
      DATA_DIR: "/data"
    volumes:
      - ./server:/app
      - ./data:/data
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    command: ["python", "-m", "app"]

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports: ["11434:11434"]
    volumes:
      - ./ollama:/root/.ollama
    restart: unless-stopped

  # Optional image backend (ComfyUI). If you enable this, set IMAGE_BACKEND=comfyui.
  # comfyui:
  #   image: eisai/comfy-ui:latest
  #   container_name: comfyui
  #   ports: ["8188:8188"]
  #   volumes:
  #     - ./comfyui/models:/opt/ComfyUI/models
  #     - ./comfyui/custom_nodes:/opt/ComfyUI/custom_nodes
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: ["gpu"]
