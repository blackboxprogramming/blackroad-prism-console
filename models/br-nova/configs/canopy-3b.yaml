model:
  name: br-canopy-3b
  d_model: 2560
  n_layers: 36
  n_heads: 40
  n_kv_heads: 10
  ffn_mult: 3.0
  vocab_size: 64000
  max_position_embeddings: 16384
  rope_theta: 2000000
  dropout: 0.0
optimizer:
  name: adamw
  lr: 0.00015
  betas: [0.9, 0.95]
  weight_decay: 0.1
  eps: 1.0e-8
scheduler:
  name: cosine
  warmup_steps: 30000
  total_steps: 800000
  min_lr_ratio: 0.05
training:
  precision: bf16
  grad_clip: 1.0
  gradient_accumulation_steps: 48
  micro_batch_size: 4
  global_batch_tokens: 20000000
  seq_length: 16384
  checkpoint_every: 1000
  eval_every: 5000
  save_milestones: [400000, 700000, 800000]
datasets:
  manifest: ../../data/manifests/canopy.json
  pack_sequences:
    max_tokens: 16384
    spillover_tolerance: 0.05
logging:
  run_card: ../../data/run_cards/canopy.json
  reflex_topic: inference.canopy
