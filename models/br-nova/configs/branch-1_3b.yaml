model:
  name: br-branch-1_3b
  d_model: 2048
  n_layers: 32
  n_heads: 32
  n_kv_heads: 8
  ffn_mult: 3.0
  vocab_size: 48000
  max_position_embeddings: 12288
  rope_theta: 1000000
  dropout: 0.0
optimizer:
  name: adamw
  lr: 0.0002
  betas: [0.9, 0.95]
  weight_decay: 0.1
  eps: 1.0e-8
scheduler:
  name: cosine
  warmup_steps: 20000
  total_steps: 600000
  min_lr_ratio: 0.05
training:
  precision: bf16
  grad_clip: 1.0
  gradient_accumulation_steps: 32
  micro_batch_size: 4
  global_batch_tokens: 12000000
  seq_length: 12288
  checkpoint_every: 1000
  eval_every: 5000
  save_milestones: [300000, 500000, 600000]
datasets:
  manifest: ../../data/manifests/branch.json
  pack_sequences:
    max_tokens: 12288
    spillover_tolerance: 0.05
logging:
  run_card: ../../data/run_cards/branch.json
  reflex_topic: inference.branch
