name: "lucidia-llamacpp"
backend: "llamacpp"
backend_params:
  host: "http://localhost:8080"  # llama.cpp server /completion endpoint
  timeout_s: 60
  max_tokens: 256
  temperature: 0.2
  top_p: 0.95
  stop: ["\n\n\n", "\n# End", "\nif __name__ == '__main__':"]
sampling:
  n_samples_per_task: 20
  tasks: "all"
prompt:
  template_path: "prompts/codex_prompt.txt"
paths:
  output_dir: "outputs"
